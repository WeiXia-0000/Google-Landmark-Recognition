{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mB8ghz3JfwnF","executionInfo":{"status":"ok","timestamp":1715221043298,"user_tz":300,"elapsed":15828,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"outputId":"9ca3a5aa-1639-46b7-930e-22068ee16663"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/cs444-final-project/project/data/small_data/train.zip\n"],"metadata":{"id":"TE1qc_-T53vj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/cs444-final-project/project/data/small_data/test.zip\n"],"metadata":{"id":"OOEMO7yJfRj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_DIR = '/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/'"],"metadata":{"id":"VjwfCSBxn9ij","executionInfo":{"status":"ok","timestamp":1715221091039,"user_tz":300,"elapsed":118,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install efficientnet-pytorch"],"metadata":{"id":"pSvijSOuGZyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install timm"],"metadata":{"id":"rHT0E2mjHWxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GrVvBkSCd0-D","executionInfo":{"status":"ok","timestamp":1715221111146,"user_tz":300,"elapsed":5803,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}}},"outputs":[],"source":["from torchvision import datasets, models, transforms\n","import os\n","import pandas as pd\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class LandmarkImageDataset(Dataset):\n","    def __init__(self, annotations_file, transform=None, id_path_mapping=\"\", is_pt=False):\n","        \"\"\"\n","        Args:\n","            annotations_file (string): Path to the CSV file with annotations.\n","            img_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","            file_extension (string, optional): Extension of the image files in the directory.\n","        \"\"\"\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.id_to_label = {str(row[0]): row[3] for row in self.img_labels.values}\n","        self.transform = transform\n","        self.id_path_mapping = pd.read_csv(id_path_mapping)\n","        self.length = len(self.id_path_mapping)\n","        self.is_pt = is_pt\n","\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        # Retrieve the image ID and path from the mapping\n","        image_id = self.id_path_mapping.iloc[idx]['id']\n","        image_path = self.id_path_mapping.iloc[idx]['path']\n","\n","        label = self.id_to_label[str(image_id)]\n","\n","        if self.is_pt:\n","            tensor = torch.load(image_path)\n","            if self.transform:\n","                tensor = self.transform(tensor)\n","            return tensor, label\n","        else:\n","            # Load the image\n","            image = Image.open(image_path)\n","            # Apply transform if any\n","            if self.transform:\n","                image = self.transform(image)\n","            return image, label\n"]},{"cell_type":"markdown","source":["# Metrics"],"metadata":{"id":"TvJTP1tJlwPV"}},{"cell_type":"code","source":["class RunningAverage:\n","    '''\n","      Computes and keeps track of the running average and the current value of metrics\n","    '''\n","    def __init__(self) -> None:\n","        self.val = 0.0\n","        self.avg = 0.0\n","        self.sum = 0.0\n","        self.count = 0\n","\n","    def update(self, val: float, n: int = 1) -> None:\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def GAP(predicts: torch.Tensor, confs: torch.Tensor, targets: torch.Tensor) -> float:\n","    \"\"\"\n","    Calculates the simplified Global Average Precision.\n","    https://www.kaggle.com/competitions/landmark-recognition-2020/overview/evaluation\n","\n","    Args:\n","        predicts (torch.Tensor): Predicted labels of shape (n_samples,).\n","        confs (torch.Tensor): Confidence scores associated with predictions, of shape (n_samples,).\n","        targets (torch.Tensor): Ground truth labels, of shape (n_samples,).\n","\n","    Returns:\n","        float: The GAP score for the given predictions and targets.\n","    \"\"\"\n","    assert len(predicts.shape) == 1 and len(confs.shape) == 1 and len(targets.shape) == 1 and predicts.shape == confs.shape and confs.shape == targets.shape\n","\n","    _, indices = torch.sort(confs, descending=True)\n","\n","    confs = confs.cpu().numpy()\n","    predicts = predicts[indices].cpu().numpy()\n","    targets = targets[indices].cpu().numpy()\n","\n","    res, true_pos = 0.0, 0\n","\n","    for i, (c, p, t) in enumerate(zip(confs, predicts, targets)):\n","        rel = int(p == t)\n","        true_pos += rel\n","\n","        res += true_pos / (i + 1) * rel\n","\n","    res /= targets.shape[0]\n","    return res"],"metadata":{"id":"YRjAjlHNly_3","executionInfo":{"status":"ok","timestamp":1715221114681,"user_tz":300,"elapsed":120,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","if torch.cuda.is_available():\n","  print(\"CUDA USED\")\n","else:\n","  print(\"CPU USED\")"],"metadata":{"id":"YNxedeone0i1","executionInfo":{"status":"ok","timestamp":1715221117821,"user_tz":300,"elapsed":125,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"29fbf983-19b1-4ad7-d674-cf6c1db22615"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA USED\n"]}]},{"cell_type":"markdown","source":["# Test and Eavluation"],"metadata":{"id":"79U1ArtoVqWC"}},{"cell_type":"code","source":["from tqdm import tqdm\n","def evaluate_model(model, dataloader, loss_fn, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0  # Track total samples processed\n","    avg_gap = RunningAverage()\n","    with torch.no_grad():\n","        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Evaluating\", leave=True)\n","        for i, (inputs, labels) in progress_bar:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, labels)\n","\n","            running_loss += loss.item()\n","            confs, preds = torch.max(outputs, 1)\n","            correct_predictions += torch.sum(preds == labels.data)\n","            total_samples += labels.size(0)\n","\n","            # Calculate GAP for this batch and update the running average\n","            avg_gap.update(GAP(preds, confs, labels))\n","\n","            # Update the progress bar with average loss and accuracy\n","            average_loss = running_loss / total_samples\n","            average_accuracy = correct_predictions.double() / total_samples\n","            progress_bar.set_postfix({\n","                'avg_loss': f'{average_loss:.4f}',\n","                'avg_acc': f'{average_accuracy:.2f}',\n","                'gap_score': f'{avg_gap.avg:.4f}'\n","            })\n","\n","\n","    total_loss = running_loss / len(dataloader)\n","    accuracy = correct_predictions.double() / len(dataloader.dataset)\n","    gap_score = avg_gap.avg\n","    # print(f'Validation Loss: {total_loss}, Accuracy: {accuracy}')\n","    return total_loss, accuracy, gap_score\n"],"metadata":{"id":"Glb72offVo7k","executionInfo":{"status":"ok","timestamp":1715221119673,"user_tz":300,"elapsed":120,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Prepares Input"],"metadata":{"id":"S_vagKL6GrEJ"}},{"cell_type":"code","source":["import os\n","# Define transformations for the training data\n","test_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","label_csv = \"/content/drive/MyDrive/cs444-final-project/project/data/small_data/small_data.csv\"\n","test_img_id_to_path_mapping = \"/content/drive/MyDrive/cs444-final-project/project/data/small_data/test_img_id_to_path_mapping_drive.csv\"\n","# Create an instance of the dataset\n","test_dataset = LandmarkImageDataset(\n","    annotations_file=label_csv,\n","    transform=test_transform,\n","    id_path_mapping=test_img_id_to_path_mapping\n",")\n","\n","\n","test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2)\n","\n","model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pth')]\n","\n","for model_file in model_files:\n","    model_path = os.path.join(MODEL_DIR, model_file)\n","    print(model_path)\n"],"metadata":{"id":"q1gQHcf5GtQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715221154287,"user_tz":300,"elapsed":28306,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"outputId":"64d09f91-912a-45f2-ad78-ddfbac5a1b14"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/resnet-50_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/vit_base_224_ep=10_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/MobileNetV2_ep=24_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/DenseNet_ep=21_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/vit_base_224_ep=30_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/resnet-18_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/resnet-50_ep30_lr=0001_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/SqueezeNet_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/DenseNet_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/efficient_net_b0_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/efficient_net_b7_best_model.pth\n","/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/swin_ep=30_best_model.pth\n"]}]},{"cell_type":"markdown","source":["# ResNet-18"],"metadata":{"id":"wg07P-8tEXhK"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/resnet-18_best_model.pth\"\n","def initialize_model(num_classes):\n","    # Load a pretrained ResNet-18 model\n","    model = models.resnet18(pretrained=True)\n","    num_ftrs = model.fc.in_features\n","    model.fc = torch.nn.Linear(num_ftrs, num_classes)\n","    return model\n","\n","# Determine the number of unique classes\n","num_classes = 25\n","model = initialize_model(num_classes=num_classes)\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'ResNet-18: Total Loss: {total_loss}, Accuracy: {accuracy}, GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIOmFr72EWSd","executionInfo":{"status":"ok","timestamp":1715220052077,"user_tz":300,"elapsed":2866,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"c0be4f87-809f-423f-d17a-8e23c795e51f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Evaluating: 100%|██████████| 184/184 [00:03<00:00, 60.27it/s, avg_loss=0.4228, avg_acc=0.67, gap_score=0.6423]\n"]},{"output_type":"stream","name":"stdout","text":["ResNet-18: Total Loss: 1.6888280155482602, Accuracy: 0.6748299319727891, GAP Score: 0.6423233695652176\n"]}]},{"cell_type":"markdown","source":["# ResNet-50"],"metadata":{"id":"PVrCpLt9E0AG"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/resnet-50_best_model.pth\"\n","def initialize_model(num_classes):\n","    # Load a pretrained ResNet-18 model\n","    model = models.resnet50(pretrained=True)\n","    num_ftrs = model.fc.in_features\n","    model.fc = torch.nn.Linear(num_ftrs, num_classes)\n","    return model\n","\n","# Determine the number of unique classes\n","num_classes = 25\n","model = initialize_model(num_classes=num_classes)\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'ResNet-50: Total Loss: {total_loss}, Accuracy: {accuracy}, GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dh5ou6iKFSwl","executionInfo":{"status":"ok","timestamp":1715220858073,"user_tz":300,"elapsed":9637,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"e7c3f3b8-b8dd-4d25-d576-cb86a2d7a622"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Evaluating: 100%|██████████| 184/184 [00:03<00:00, 59.89it/s, avg_loss=0.0992, avg_acc=0.88, gap_score=0.8697]"]},{"output_type":"stream","name":"stdout","text":["ResNet-50: Total Loss: 1.1964374558253558, Accuracy: 0.7429931972789115, GAP Score: 0.7496784420289856\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# Efficient Net B0"],"metadata":{"id":"OneHTgvjFvs_"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","import efficientnet_pytorch\n","class EfficientNetEncoderHead(nn.Module):\n","    \"\"\"\n","    Defining an EfficientNet encoder head for image classification.\n","\n","    Attributes:\n","        depth (int): The depth version of the EfficientNet.\n","        num_classes (int): The number of classes for the classifier output.\n","        base (EfficientNet): The base EfficientNet model preloaded with pretrained weights.\n","        avg_pool (nn.AdaptiveAvgPool2d): Adaptive average pooling to reduce spatial dimensions to 1x1.\n","        output_filter (int): The number of output features from the EfficientNet.\n","        classifier (nn.Linear): The linear classifier that outputs the class probabilities.\n","\n","    Methods:\n","        forward(x): Defines the forward pass of the model.\n","    \"\"\"\n","    def __init__(self, depth, num_classes):\n","        super(EfficientNetEncoderHead, self).__init__()\n","        self.depth = depth\n","        self.base = efficientnet_pytorch.EfficientNet.from_pretrained(f'efficientnet-b{self.depth}')\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.output_filter = self.base._fc.in_features\n","        self.classifier = nn.Linear(self.output_filter, num_classes)\n","    def forward(self, x):\n","        x = self.base.extract_features(x)\n","        x = self.avg_pool(x).squeeze(-1).squeeze(-1)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/efficient_net_b0_best_model.pth\"\n","\n","# Determine the number of unique classes\n","num_classes = 25\n","model = EfficientNetEncoderHead(depth=0, num_classes=num_classes)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'EfficientNet-b0: Total Loss: {total_loss}, Accuracy: {accuracy}, GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfZW0bjdFwJN","executionInfo":{"status":"ok","timestamp":1715220431736,"user_tz":300,"elapsed":9320,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"aed45699-f302-410e-b108-63fdf6043811"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","100%|██████████| 20.4M/20.4M [00:00<00:00, 363MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b0\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 184/184 [00:03<00:00, 48.11it/s, avg_loss=0.2300, avg_acc=0.81, gap_score=0.7980]\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNet-b0: Total Loss: 0.9187016650719907, Accuracy: 0.8136054421768707, GAP Score: 0.7980072463768116\n"]}]},{"cell_type":"markdown","source":["# Efficient Net B7"],"metadata":{"id":"UwB2r9fpGz4r"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","import efficientnet_pytorch\n","class EfficientNetEncoderHead(nn.Module):\n","    \"\"\"\n","    Defining an EfficientNet encoder head for image classification.\n","\n","    Attributes:\n","        depth (int): The depth version of the EfficientNet.\n","        num_classes (int): The number of classes for the classifier output.\n","        base (EfficientNet): The base EfficientNet model preloaded with pretrained weights.\n","        avg_pool (nn.AdaptiveAvgPool2d): Adaptive average pooling to reduce spatial dimensions to 1x1.\n","        output_filter (int): The number of output features from the EfficientNet.\n","        classifier (nn.Linear): The linear classifier that outputs the class probabilities.\n","\n","    Methods:\n","        forward(x): Defines the forward pass of the model.\n","    \"\"\"\n","    def __init__(self, depth, num_classes):\n","        super(EfficientNetEncoderHead, self).__init__()\n","        self.depth = depth\n","        self.base = efficientnet_pytorch.EfficientNet.from_pretrained(f'efficientnet-b{self.depth}')\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.output_filter = self.base._fc.in_features\n","        self.classifier = nn.Linear(self.output_filter, num_classes)\n","    def forward(self, x):\n","        x = self.base.extract_features(x)\n","        x = self.avg_pool(x).squeeze(-1).squeeze(-1)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/efficient_net_b7_best_model.pth\"\n","\n","# Determine the number of unique classes\n","num_classes = 25\n","model = EfficientNetEncoderHead(depth=7, num_classes=num_classes)\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'EfficientNet-b7: Total Loss: {total_loss}, Accuracy: {accuracy}, GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfXF7E4TGxCU","executionInfo":{"status":"ok","timestamp":1715220667998,"user_tz":300,"elapsed":11347,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"d026a018-7a85-49bb-eaab-9e63786be920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Evaluating: 100%|██████████| 184/184 [00:09<00:00, 18.56it/s, avg_loss=0.2228, avg_acc=0.83, gap_score=0.8196]\n"]},{"output_type":"stream","name":"stdout","text":["EfficientNet-b7: Total Loss: 0.8899004632821546, Accuracy: 0.8812925170068026, GAP Score: 0.8696331521739131\n"]}]},{"cell_type":"markdown","source":["# ViT"],"metadata":{"id":"16iQ0oZcHIbW"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import timm\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/vit_base_224_ep=30_best_model.pth\"\n","model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=25)\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'ViT: Total Loss: {total_loss}, Accuracy: {accuracy}, GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["73bc871e03e04fb0886f33c0f5f6c1a7","c35c83fd91de46708b8cc5c2c7aa82ec","06d54382b8f747e682c0985f2add8516","3feb33cf17754b7ba662d4e784d6ac82","8e16d0e5fe47450c8701448a581ff6c1","3070c2a7364c4f7fbdf0c6a49c7f14d6","d4e26adf1a454b8bb8498e46913ca154","cfef8233dbee49c994352eed36a873b7","418ed3054ef744619d2f17f31038305c","08de0808be5e4e619a986589e7f9ec74","685cd0439cf84fdcb14ccc2e111bff81"]},"id":"kR8xBGV4HMB1","executionInfo":{"status":"ok","timestamp":1715220695200,"user_tz":300,"elapsed":16920,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"470eac26-a114-4251-8ea5-02fb5e0e7e92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73bc871e03e04fb0886f33c0f5f6c1a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 184/184 [00:04<00:00, 42.93it/s, avg_loss=0.5278, avg_acc=0.74, gap_score=0.7137]"]},{"output_type":"stream","name":"stdout","text":["ViT: Total Loss: 1.082392482979726, Accuracy: 0.8387755102040816, GAP Score: 0.8136548913043481\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# SWIN Transformer"],"metadata":{"id":"2jV9t8ETHzlS"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import timm\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/swin_ep=30_best_model.pth\"\n","model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=25)\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'SWIN: GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unkLjBJuHvCN","executionInfo":{"status":"ok","timestamp":1715220809226,"user_tz":300,"elapsed":4924,"user":{"displayName":"Ruipeng Han","userId":"01141656125453541823"}},"outputId":"70fbd798-94e0-497a-9d59-aec93ca55e8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 184/184 [00:03<00:00, 55.67it/s, avg_loss=0.2962, avg_acc=0.81, gap_score=0.7935]\n"]},{"output_type":"stream","name":"stdout","text":["SWIN: GAP Score: 0.7934782608695654\n"]}]},{"cell_type":"markdown","source":["# MobileNetV2\n"],"metadata":{"id":"xpm8aY_FJZkk"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","\n","model_path = \"/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/MobileNetV2_ep=24_best_model.pth\"\n","model = models.mobilenet_v2(pretrained=True)\n","model.classifier = nn.Sequential(\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=model.classifier[1].in_features, out_features=25)\n",")\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'SWIN: GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkxQqS4aJeBa","executionInfo":{"status":"ok","timestamp":1715221472442,"user_tz":300,"elapsed":6483,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"outputId":"3dd32b29-886f-4382-efa7-de3029f1569c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Evaluating: 100%|██████████| 184/184 [00:04<00:00, 39.26it/s, avg_loss=0.3947, avg_acc=0.70, gap_score=0.6675]"]},{"output_type":"stream","name":"stdout","text":["SWIN: GAP Score: 0.6674592391304349\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# DenseNet\n"],"metadata":{"id":"gO4rzYobKqA8"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","\n","model_path = '/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/DenseNet_best_model.pth'\n","model = models.densenet121(pretrained=True)\n","model.classifier = nn.Linear(model.classifier.in_features, 25)\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'SWIN: GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yxYc-CzyKpqB","executionInfo":{"status":"ok","timestamp":1715221676975,"user_tz":300,"elapsed":8516,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"outputId":"7bdc3c28-f69a-48c5-82eb-2fc4b54495b3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","100%|██████████| 30.8M/30.8M [00:00<00:00, 153MB/s]\n","Evaluating: 100%|██████████| 184/184 [00:05<00:00, 35.73it/s, avg_loss=0.2915, avg_acc=0.73, gap_score=0.7077]"]},{"output_type":"stream","name":"stdout","text":["SWIN: GAP Score: 0.7076539855072465\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# SqueezeNet"],"metadata":{"id":"ip3Xw4_fLfPl"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","\n","model_path = '/content/drive/MyDrive/cs444-final-project/project/models/baselines/saved_models/SqueezeNet_best_model.pth'\n","model = models.squeezenet1_1(pretrained=True)\n","final_conv = nn.Conv2d(512, 25, kernel_size=(1,1))\n","model.classifier[1] = final_conv\n","model.num_classes = 25\n","\n","model.load_state_dict(torch.load(model_path))\n","model.to(device)\n","total_loss, accuracy, gap_score = evaluate_model(model, test_loader, loss_fn, device)\n","print(f'SWIN: GAP Score: {gap_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-7iyuCKLbss","executionInfo":{"status":"ok","timestamp":1715221754447,"user_tz":300,"elapsed":4905,"user":{"displayName":"Wei Xia","userId":"05827467599941451869"}},"outputId":"3a25c5c2-aa02-40bd-9249-861b733898fa"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n","100%|██████████| 4.73M/4.73M [00:00<00:00, 84.9MB/s]\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Evaluating: 100%|██████████| 184/184 [00:03<00:00, 55.63it/s, avg_loss=0.4734, avg_acc=0.64, gap_score=0.6042]"]},{"output_type":"stream","name":"stdout","text":["SWIN: GAP Score: 0.604166666666667\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Plot"],"metadata":{"id":"MJ0UlSLEyq8q"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot_metrics_acc(train_losses, train_accuracies, test_accuracies):\n","    plt.figure(figsize=(12, 6))\n","\n","    # Create the first axis for the training loss\n","    ax1 = plt.gca()  # Get current axis\n","    line1, = ax1.plot(train_losses, label='Training Loss', color='red', marker='o')\n","    ax1.set_xlabel('Epochs')  # Common x-axis label\n","    ax1.set_ylabel('Loss', color='red')  # Set y-axis label for loss\n","    ax1.tick_params(axis='y', labelcolor='red')  # Set the color of y-axis ticks to red\n","\n","    # Create a second y-axis for training accuracy using twinx()\n","    ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis\n","    line2, = ax2.plot(train_accuracies, label='Training Accuracy', color='blue', marker='x')\n","    ax2.set_ylabel('Accuracy', color='blue')  # Set y-axis label for accuracy\n","    ax2.tick_params(axis='y', labelcolor='blue')  # Set the color of y-axis ticks to blue\n","\n","    # Create a third y-axis using a new set of axes positioned to the right\n","    ax3 = ax1.twinx()  # New axis that shares x-axis\n","    ax3.spines['right'].set_position(('outward', 60))  # Move the third axis out to avoid overlap\n","    line3, = ax3.plot(test_accuracies, label='Test Accuracy', color='green', marker='s')\n","    ax3.set_ylabel('Test Accuracy', color='green')  # Set y-axis label for test accuracy\n","    ax3.tick_params(axis='y', labelcolor='green')  # Set the color of y-axis ticks to green\n","\n","    # Title and grid\n","    plt.title('ResNet-18: Training Loss, Training Accuracy, and Test Accuracy')\n","    ax1.grid(True)\n","\n","    # Create legends and position them appropriately\n","    lines = [line1, line2, line3]\n","    labels = [l.get_label() for l in lines]\n","    plt.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)  # Adjust legend position\n","\n","    plt.show()\n"],"metadata":{"id":"bttaCz5ut-Sb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save logs"],"metadata":{"id":"Si3xvDJG0AE_"}},{"cell_type":"code","source":["import json\n","\n","data = {\n","    \"train_loss\": train_losses,\n","    \"train_accuracy\": train_acc,\n","    \"train_gap\": train_gap,\n","    \"test_accuracy\": test_acc\n","}\n","LOG_FILE\n","with open(LOG_FILE, \"w\") as json_file:\n","    json.dump(data, json_file, indent=4)\n"],"metadata":{"id":"7kiVO10rzWjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y1MJgX7w0QA5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1D8s6WUoduWEbo4m284_MdA7pG7qsLjiZ","timestamp":1715218373539}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73bc871e03e04fb0886f33c0f5f6c1a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c35c83fd91de46708b8cc5c2c7aa82ec","IPY_MODEL_06d54382b8f747e682c0985f2add8516","IPY_MODEL_3feb33cf17754b7ba662d4e784d6ac82"],"layout":"IPY_MODEL_8e16d0e5fe47450c8701448a581ff6c1"}},"c35c83fd91de46708b8cc5c2c7aa82ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3070c2a7364c4f7fbdf0c6a49c7f14d6","placeholder":"​","style":"IPY_MODEL_d4e26adf1a454b8bb8498e46913ca154","value":"model.safetensors: 100%"}},"06d54382b8f747e682c0985f2add8516":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfef8233dbee49c994352eed36a873b7","max":346284714,"min":0,"orientation":"horizontal","style":"IPY_MODEL_418ed3054ef744619d2f17f31038305c","value":346284714}},"3feb33cf17754b7ba662d4e784d6ac82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08de0808be5e4e619a986589e7f9ec74","placeholder":"​","style":"IPY_MODEL_685cd0439cf84fdcb14ccc2e111bff81","value":" 346M/346M [00:00&lt;00:00, 414MB/s]"}},"8e16d0e5fe47450c8701448a581ff6c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3070c2a7364c4f7fbdf0c6a49c7f14d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4e26adf1a454b8bb8498e46913ca154":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfef8233dbee49c994352eed36a873b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"418ed3054ef744619d2f17f31038305c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08de0808be5e4e619a986589e7f9ec74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685cd0439cf84fdcb14ccc2e111bff81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}